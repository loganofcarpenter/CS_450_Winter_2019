{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/LoganTheKid/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py as h5\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rand\n",
    "\n",
    "import ML_func_defs as ML\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prove 06, 07, and 08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class net definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class net():\n",
    "    def __init__(self,train_data, target_data, num_lay, num_nodes, update_type='batch',\n",
    "             g='sigmoid', plot_figs=False, momentum=False, momentum_alpha=.9,\n",
    "             max_it=100**10,eta_0=.1,eta_decay=False, eta_half_life=10000,\n",
    "             regression=False):\n",
    "        self.train_data = train_data\n",
    "        self.target     = target_data\n",
    "        self.input_dim  = len(train_data[0])-1\n",
    "        self.output_dim = len( set( list( target_data.flat ) ) )\n",
    "        self.num_lay    = num_lay\n",
    "        self.num_nodes  = num_nodes\n",
    "        \n",
    "        self.update_type = update_type            \n",
    "        self.g_function = g\n",
    "    \n",
    "        self.regression = regression\n",
    "        if regression:\n",
    "            self.output_dim = 1\n",
    "        \n",
    "        self.init_weights() \n",
    "        self.momentum   = momentum\n",
    "        self.momentum_a = momentum_alpha\n",
    "        self.momentum_w = [np.zeros(( w.shape[0],w.shape[1] )) for w in self.weights ]\n",
    "        self.max_it     = max_it\n",
    "        self.plot_figs  = plot_figs\n",
    "        self.train_acc, self.train_err = [], []\n",
    "        self.val_acc  , self.val_err   = [], []\n",
    "        self.eta        = eta_0\n",
    "        self.eta_0      = eta_0\n",
    "        self.eta_decay  = eta_decay\n",
    "        self.eta_decay_rate = self.get_half_life( eta_half_life )\n",
    "        \n",
    "        \n",
    "    def g_func(self, h):\n",
    "        if self.g_function == 'sigmoid':\n",
    "            return (1 + np.exp(-h))**-1.\n",
    "        elif self.g_function == 'tanh':\n",
    "            return np.tanh(h)\n",
    "        else:\n",
    "            return 'whoopsies: self.g_func()'\n",
    "\n",
    "    def g_prime(self, h):\n",
    "        if self.g_function == 'sigmoid':\n",
    "            return self.g_func(h) * ( 1 - self.g_func(h) )\n",
    "        elif self.g_function == 'tanh':\n",
    "            return ( np.sech(h) )**2.\n",
    "        else:\n",
    "            return 'whoopsies: self.g_prime()'\n",
    "    \n",
    "    def init_weights(self):\n",
    "        BI_NODE = 1\n",
    "        self.weights = []\n",
    "        for i in np.arange(self.num_lay+1):\n",
    "            if i == 0:\n",
    "                self.weights.append(rand.normal( loc=0.0, scale=0.15,\n",
    "                               size=(self.num_nodes[i], self.input_dim+BI_NODE)))\n",
    "            else:\n",
    "                self.weights.append(rand.normal(loc=0.0,scale=0.15,\n",
    "                               size=(self.num_nodes[i], self.num_nodes[i-1]+BI_NODE)))\n",
    "\n",
    "    def stack_and_trans(self, var):\n",
    "        new_var = var[0]\n",
    "        for i in np.arange(1, len(var)):\n",
    "            new_var = np.vstack(( new_var, var[i] ))\n",
    "        return np.transpose( new_var )\n",
    "\n",
    "    def add_bias(self, var):\n",
    "        if var[0].ndim == 0:\n",
    "            tmp_var = np.zeros(( len(var), 2 )) - 1\n",
    "        else:\n",
    "            tmp_var = np.zeros(( len(var), len(var[0])+1 )) - 1\n",
    "        tmp_var[:,:-1] = var\n",
    "        return tmp_var\n",
    "        \n",
    "    def forward_prop(self, data):\n",
    "        h_temp = [ np.dot( data,self.weights[0][x]) \\\n",
    "                           for x in np.arange(len(self.weights[0]))]\n",
    "        h = self.stack_and_trans(h_temp)\n",
    "        \n",
    "        a_temp = [ self.g_func(h) for x in range(len(h))]\n",
    "        a_lst  = [ self.add_bias( a_temp[0] ) ]\n",
    "\n",
    "        # loop though all the weights (except the first layer)\n",
    "        for i,ws in enumerate(self.weights[1:]):\n",
    "            # get h's for the next layer\n",
    "            h = self.stack_and_trans( np.array([ np.dot(a_lst[-1],ws[x]) \\\n",
    "                                                 for x in range(len(ws)) ]) )\n",
    "            # gets the next layer's a's\n",
    "            a_temp = np.array([self.g_func(h[x]) for x in range(len(h))])\n",
    "            \n",
    "            \n",
    "            # logic to make sure the output layer doesn't have a bias node\n",
    "            if i == self.num_lay - 1:\n",
    "                # if it's a regression problem get a linear output\n",
    "                if self.regression:\n",
    "                    a_temp = np.array(sum(h))                    \n",
    "                a_lst.append( a_temp )\n",
    "            else:\n",
    "                # adds the bias node\n",
    "                a_lst.append( self.add_bias( a_temp ) )\n",
    "                \n",
    "        return a_lst\n",
    "\n",
    "    \n",
    "    def get_deltas(self, targ, a):\n",
    "        # reverse the weights and a's\n",
    "        rev_w = self.weights[::-1]\n",
    "        rev_a = a[::-1]\n",
    "\n",
    "        # handle the output layer first because it's a different equation\n",
    "        output_delta = np.array( rev_a[0] * (1-rev_a[0]) * (rev_a[0]-targ) )\n",
    "        # This handles the case if there's only one output node. \n",
    "        if output_delta.ndim == 1:\n",
    "            output_delta = np.array([np.array([d]) for d in output_delta])\n",
    "        # deltas currently has dimensionality 1 x (N x M) where N is the \n",
    "        #        number of data entries in the training set and M is the\n",
    "        #        number of output nodes.\n",
    "        deltas = [ output_delta ]\n",
    "\n",
    "        for j in np.arange(1, len(rev_w)):\n",
    "            i = j - 1\n",
    "            # delete the bias weights because they don't effect the next deltas\n",
    "            # Note: w_nobi has dimensionality (AxB) where A is the number of \n",
    "            #       nodes in layer i and B is the number of nodes in layer j.\n",
    "            w_nobi = np.delete(rev_w[i], len(rev_w[i])-1, 1) \n",
    "\n",
    "            # w_nobi is set up so that the colmuns correlate to a node \n",
    "            #     from layer j - and a row correlates to all the entries to a\n",
    "            #     node in layer k. But we want all the weights that leave a \n",
    "            #     node in layer j, so it needs to be transposed.\n",
    "            w_tr   = np.transpose( w_nobi )\n",
    "\n",
    "            # delete the bias activations because they don't effect \n",
    "            #        the next deltas.\n",
    "            a_nobi = np.delete(rev_a[j], len(rev_a[j][0])-1, 1)\n",
    "            a_part = a_nobi * (1 - a_nobi)\n",
    "\n",
    "            # make a temporary delta\n",
    "            delta_t = deltas[-1]\n",
    "            #handles the case that there is only one node by putting delta_t\n",
    "            #        into the right format.  \n",
    "            if delta_t.ndim == 1:\n",
    "                delta_t = np.array([np.array([d]) for d in delta_t])\n",
    "                \n",
    "            # Get the sum of the deltas from layer k and the weights that connect \n",
    "            #     layer k to j. Currently the rows of delta_part correlate to the.\n",
    "            delta_part = np.array( [ np.dot(delta_t,w_tr[x]) for x in range(len(w_tr))] )\n",
    "\n",
    "            new_delta = np.array( a_part * np.transpose( delta_part ) ) \n",
    "            deltas.append( new_delta )\n",
    "\n",
    "        return deltas[::-1]\n",
    "    \n",
    "    \n",
    "    def weight_update(self, data, targ, a):\n",
    "        wts = deepcopy( self.weights )\n",
    "        deltas = self.get_deltas( targ, a )\n",
    "        change_w = [ np.zeros(( w.shape[0],w.shape[1] )) for w in self.weights ]\n",
    "        a.insert(0, data)\n",
    "\n",
    "        for j,w in enumerate(self.weights):\n",
    "            if self.update_type == 'batch':\n",
    "                if self.momentum:\n",
    "                        self.momentum_w[j] += self.momentum_a * change_w[j]                      \n",
    "                for x,a1 in enumerate( a[j] ):\n",
    "                    # Note: a1 has dimentionality (AxB) where A is the length of\n",
    "                    #       training data set and B is the number of nodes in\n",
    "                    #       layer i.\n",
    "                    a1, delta = np.array( a1 ), np.array( deltas[j][x] )\n",
    "                    del_X_a = np.array([ a1*delt for delt in delta ])\n",
    "                    change_w[j] -= self.eta * del_X_a  \n",
    "            elif self.update_type == 'sequential':\n",
    "                if self.momentum:\n",
    "                        self.momentum_w[j] += self.momentum_a * change_w[j]  \n",
    "                        \n",
    "                a1, delta = np.array( a[j].flat ), np.array( deltas[j].flat )\n",
    "                del_X_a = np.array([ a1*delt for delt in delta ])\n",
    "                change_w[j] -= self.eta * del_X_a\n",
    "            else:\n",
    "                print(\"ERROR - weight_update(): \")\n",
    "                print(\"        update_type must be batch or sequential.\")\n",
    "                return 0\n",
    "            \n",
    "            wts[j] += change_w[j] + self.momentum_w[j]\n",
    "        return wts\n",
    "    \n",
    "    def stop_criteria(self, count, val_err):\n",
    "        if count < 1500:\n",
    "            # Too soon to stop\n",
    "            return True\n",
    "        elif sum(val_err[-4:-2]) > sum(val_err[-2:]):\n",
    "            # Continue so long as the average validation continues to go down\n",
    "            return True\n",
    "        else:\n",
    "            # stops learning\n",
    "            return False\n",
    "        \n",
    "    \n",
    "    def get_accuracy(self, data, targs):\n",
    "        if self.regression:\n",
    "            # gets the Mean Absolute Percent Error for regression problems\n",
    "            acc = len(data)**-1 * np.sum(np.array([ (t-data[x])/t for x,t in enumerate(targs)]) )\n",
    "        else:\n",
    "            pred      = np.zeros(( len(data), len(targs[0]) ))\n",
    "            # the [-1] grabs just the output layer\n",
    "            output    = self.forward_prop( data )[-1] \n",
    "            pred_args = np.argmax( output, axis=1 )\n",
    "\n",
    "            corr_cnt = 0\n",
    "            for i in np.arange( len(pred) ):\n",
    "                pred[i, pred_args[i]] = 1  # make the prediction array\n",
    "                if np.argmax(pred[i]) == np.argmax(targs[i]):\n",
    "                    corr_cnt += 1 \n",
    "            acc = corr_cnt/len( pred )\n",
    "        \n",
    "        return acc\n",
    "    \n",
    "    def get_error(self, data, targs):\n",
    "        output = self.forward_prop( data )[-1]\n",
    "        if self.regression:\n",
    "            return .5 * np.sum( (output - targs)**2 )\n",
    "        else:\n",
    "            prediction = np.zeros(( len(data), len(targs[0]) )) \n",
    "            pred_arg   = np.argmax( output, axis=1 )\n",
    "            for i in np.arange( len(prediction) ):\n",
    "                prediction[i, pred_arg[i]] = 1\n",
    "\n",
    "            return .5 * np.sum( (prediction - targs)**2 )\n",
    "    \n",
    "    def plot_one_fig(self, data, name, color='b'):\n",
    "        plt.figure(name)\n",
    "        plt.plot(data, color)\n",
    "        plt.title(name)\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_figures(self, datas, names, colors=['o','b','g','k']):\n",
    "        fig, axs = plt.subplots(nrows=2, ncols=2)\n",
    "        ax = axs.flatten()\n",
    "        for i in range(4):\n",
    "            ax[i].plot(datas[i], colors[i])\n",
    "            ax[i].set_title(names[i])\n",
    "        fig.subplots_adjust(left=0.2, hspace=0.4, wspace=.3)\n",
    "\n",
    "    def eta_decay_func(self, count):\n",
    "        return self.eta_0 * np.exp( - self.eta_decay_rate * count )\n",
    "    \n",
    "    def get_half_life(self, count):\n",
    "        return - np.exp( .5 ) * count**-1.\n",
    "            \n",
    "    def Test(self, test_data, test_targs):\n",
    "        acc = self.get_accuracy(test_data, test_targs)\n",
    "        err = self.get_error(   test_data, test_targs)\n",
    "        print(\"\\nFor the Test set:\")\n",
    "        print(\"The error =\",err,\"    And the accuracy =\",acc)\n",
    "    \n",
    "    def train_net(self, val_data, val_test):\n",
    "        a0, count= 0, 1\n",
    "\n",
    "        while self.stop_criteria(count,self.val_err) and (count < self.max_it): \n",
    "            \n",
    "            if self.update_type == 'batch':            \n",
    "                a = self.forward_prop( self.train_data )\n",
    "                self.weights = self.weight_update(self.train_data, self.target, a)\n",
    "            elif self.update_type == 'sequential':\n",
    "                for x,d in enumerate(self.train_data):\n",
    "                    a = self.forward_prop( d )\n",
    "                    self.weights = self.weight_update(d, self.target[x], a)\n",
    "            else:\n",
    "                print(\"ERROR: update_type must be batch or sequential.\")\n",
    "                return 0\n",
    "            \n",
    "            if count % 50 == 0:\n",
    "                self.val_acc.append( self.get_accuracy(val_data, val_test) )\n",
    "                self.val_err.append( self.get_error(val_data, val_test) )\n",
    "                self.train_err.append( self.get_error(self.train_data, self.target) )\n",
    "                self.train_acc.append(self.get_accuracy(self.train_data,self.target))\n",
    "                \n",
    "                if count % 1000 == 0:\n",
    "                    print(\"Iteration \",count,\"  Train accuracy =\",self.train_acc[-1])\n",
    "                    a1 = a[0]\n",
    "                    print(\"normed difference of the activation =\",np.linalg.norm(a1-a0))\n",
    "                    a0 = a[0]\n",
    "            \n",
    "            if self.eta_decay:\n",
    "                self.eta = self.eta_decay_func( count )\n",
    "            count += 1\n",
    "        \n",
    "        if self.plot_figs:\n",
    "            data = [self.val_err, self.val_acc, self.train_err, self.train_acc]\n",
    "            name = [\"Validation Set Error\", \"Validation Set Accuracy\",\\\n",
    "                    \"Training Set Error\", \"Training Set Accuracy\" ]\n",
    "            colors = ['orange', 'green', 'blue', 'k']\n",
    "            self.plot_figures(data, name, colors=colors)\n",
    "            #self.plot_one_fig(self.val_err, \"Validation Set Error\", color='orange')\n",
    "            #self.plot_one_fig(self.val_acc, \"Validation Set Accuracy\", color='green')\n",
    "            #self.plot_one_fig(self.train_err, \"Training Set Error\", color='blue')\n",
    "            #self.plot_one_fig(self.train_acc, \"Training Set Accuracy\", color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many hidden layers do you want (must be an integer)? 1\n",
      "How many nodes in hidden layer  1 ? \n",
      "3\n",
      "Iteration  1000   Train accuracy = 0.9275\n",
      "normed difference of the activation = 20.0\n",
      "\n",
      "For the Test set:\n",
      "The error = 15.0     And the accuracy = 0.88\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVYAAAEICAYAAAAEK9wEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################\n",
    "#### GET THE LAYER INFO\n",
    "DATA_OUTPUT_DIM = 3\n",
    "num_lay = int( input(\"How many hidden layers do you want (must be an integer)? \") )\n",
    "\n",
    "num_nodes = [ ]\n",
    "for i in np.linspace(1, num_lay, num_lay):\n",
    "    print(\"How many nodes in hidden layer \",int(i),\"? \")\n",
    "    num_nodes.append(int( input() ) )\n",
    "num_nodes.append( DATA_OUTPUT_DIM )\n",
    "#########################\n",
    "#### GET THE BALANCE DATA\n",
    "b_data = open(\"balance_scale.txt\",\"rt\")\n",
    "contents = b_data.readlines()\n",
    "b_data.close()\n",
    "\n",
    "# Turns the string version of the data into floats in a 150x4 array\n",
    "bal_data = np.array([[float(x.split(',')[1]),\n",
    "                      float(x.split(',')[2]),\n",
    "                      float(x.split(',')[3]),\n",
    "                      float((x.split(',')[4]).split('\\n')[0])] for x in contents])\n",
    "\n",
    "## gets the iris names in stings 'setosa', 'versicolor', and 'virginica'\n",
    "bal_target = [ x.split(',')[0] for x in contents ]\n",
    "## dictionary to make recasting iris names intointegers easy\n",
    "targ_vals = {'L':0, 'B':1, 'R':2}\n",
    "## loop to recasting iris names into integers\n",
    "for i,name in enumerate(bal_target):\n",
    "    bal_target[i] = targ_vals[name]\n",
    "    \n",
    "###########################\n",
    "#### SET UP THE NORMED DATA\n",
    "norm_data = preprocessing.normalize(bal_data)\n",
    "data = np.zeros(( len(norm_data), len(norm_data[0])+1 )) - 1\n",
    "data[:,:-1] = norm_data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(norm_data, bal_target, \n",
    "                                    test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                    test_size=0.2, random_state=1)\n",
    "\n",
    "def process_targets(target):\n",
    "    new_targ = np.zeros(( len(target), 3 ))\n",
    "    for i in np.arange( len(target) ):\n",
    "        t = target[i]\n",
    "        new_targ[i,t] = 1\n",
    "    return new_targ\n",
    "\n",
    "y_train = process_targets( y_train )\n",
    "y_val   = process_targets( y_val   )\n",
    "y_test  = process_targets( y_test  )\n",
    "\n",
    "Net = net(X_train, y_train, num_lay, num_nodes,update_type='batch',\\\n",
    "          momentum=True,plot_figs=True, max_it=10000)\n",
    "#Net = net(X_train, y_train, num_lay, num_nodes,update_type='batch',\\\n",
    "#           plot_figs=True,eta_decay=True)\n",
    "#Net = net(X_train, y_train, num_lay, num_nodes,update_type='sequential',\\\n",
    "#           plot_figs=True, eta_decay=True)\n",
    "#Net = net(X_train, y_train, num_lay, num_nodes,update_type='sequential',\\\n",
    "#          momentum=True,plot_figs=True)\n",
    "Net.train_net(X_val, y_val)\n",
    "Net.Test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Iris Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DATA_OUTPUT_DIM = 3\n",
    "num_lay = int( input(\"How many hidden layers do you want (must be an integer)? \") )\n",
    "\n",
    "num_nodes = [ ]\n",
    "for i in np.linspace(1, num_lay, num_lay):\n",
    "    print(\"How many nodes in hidden layer \",int(i),\"? \")\n",
    "    num_nodes.append(int( input() ) )\n",
    "num_nodes.append( DATA_OUTPUT_DIM )\n",
    "\n",
    "iris      = datasets.load_iris()\n",
    "targs     = iris.target\n",
    "norm_data = preprocessing.normalize(iris.data)\n",
    "\n",
    "data = np.zeros(( len(norm_data), len(norm_data[0])+1 )) - 1\n",
    "data[:,:-1] = norm_data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(norm_data, targs, \n",
    "                                    test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                    test_size=0.2, random_state=1)\n",
    "\n",
    "def process_iris_targets(target):\n",
    "    new_targ = np.zeros(( len(target), 3 ))\n",
    "    for i in np.arange( len(target) ):\n",
    "        t = target[i]\n",
    "        new_targ[i,t] = 1\n",
    "    return new_targ\n",
    "\n",
    "y_train = process_iris_targets( y_train )\n",
    "y_val   = process_iris_targets( y_val   )\n",
    "y_test  = process_iris_targets( y_test  )\n",
    "\n",
    "Net = net(X_train, y_train, num_lay, num_nodes,update_type='batch',\\\n",
    "          momentum=True,plot_figs=True, max_it=10000)\n",
    "#Net = net(X_train, y_train, num_lay, num_nodes,update_type='batch',\\\n",
    "#           plot_figs=True,eta_decay=True)\n",
    "#Net = net(X_train, y_train, num_lay, num_nodes,update_type='sequential',\\\n",
    "#           plot_figs=True, eta_decay=True)\n",
    "#Net = net(X_train, y_train, num_lay, num_nodes,update_type='sequential',\\\n",
    "#          momentum=True,plot_figs=True)\n",
    "Net.train_net(X_val, y_val)\n",
    "Net.Test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Regressioni Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Car Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gross_file = open('auto-mpd.txt','r')\n",
    "ugly_lines = gross_file.readlines()\n",
    "\n",
    "mpg, cylinders, displacement, horsepower, weight = [], [], [], [], []\n",
    "acceleration, model_year, origin, car_name = [], [], [], []\n",
    "\n",
    "for lin in ugly_lines:\n",
    "    # The if gets rid of missing data now\n",
    "    if lin.split('      ')[1] != '?':\n",
    "        mpg.append( float(lin.split('  ')[0]) )\n",
    "        cylinders.append( int( lin.split('   ')[1] ) )\n",
    "        displacement.append( float( lin.split('   ')[2] ) )\n",
    "        horsepower.append( float( lin.split('      ')[1] ) )\n",
    "        weight.append( float( lin.split('      ')[2] ) )\n",
    "        acceleration.append( float( lin.split('      ')[3].split('   ')[0] ) )\n",
    "        model_year.append( int( lin.split('      ')[3].split('   ')[1].split('  ')[0] ) )\n",
    "        origin.append( int( lin.split('      ')[3].split('   ')[1].split('  ')[1].split('\\t')[0] ) )\n",
    "        car_name.append( lin.split('\"')[1] )\n",
    "        \n",
    "\n",
    "auto_data = pd.DataFrame( {#'mpg':          mpg         ,\n",
    "                           'cylinders':    cylinders   , \n",
    "                           'displacement': displacement,\n",
    "                           'horsepower':   horsepower  ,\n",
    "                           'weight':       weight      ,\n",
    "                           'acceleration': acceleration,\n",
    "                           'model_year':   model_year  ,\n",
    "                           'origin':       origin      ,\n",
    "                           'car_name':     car_name     \n",
    "                           } )\n",
    "\n",
    "# the string car_name into an integer\n",
    "auto_data.car_name.value_counts()\n",
    "auto_data.car_name = auto_data.car_name.astype('category')\n",
    "auto_data['car_name_cat'] = auto_data.car_name.cat.codes\n",
    "auto_data = auto_data.drop( labels = \"car_name\" , axis=1)\n",
    "\n",
    "auto_train  = np.array( auto_data )\n",
    "auto_target = np.array( mpg )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Yacht Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gross_file = open('yacht_data.txt','r')\n",
    "ugly_lines = gross_file.readlines()\n",
    "\n",
    "f1, f2, f3, f4, f5, f6, targ = [], [], [], [], [], [], []\n",
    "\n",
    "for lin in ugly_lines:\n",
    "    f1.append( float(lin.split(' ')[0] ) )\n",
    "    f2.append( float(lin.split(' ')[1] ) )\n",
    "    f3.append( float(lin.split(' ')[2] ) )\n",
    "    f4.append( float(lin.split(' ')[3] ) )\n",
    "    f5.append( float(lin.split(' ')[4] ) )\n",
    "    f6.append( float(lin.split(' ')[5] ) )\n",
    "    targ.append( float((lin.split(' ')[6]).split('\\n')[0]) )\n",
    "        \n",
    "\n",
    "yacht_data = pd.DataFrame( {\n",
    "                           'f1':f1, \n",
    "                           'f2':f2, \n",
    "                           'f3':f3,\n",
    "                           'f4':f4, \n",
    "                           'f5':f5, \n",
    "                           'f6':f6\n",
    "                           } )\n",
    "\n",
    "yacht_train  = np.array( yacht_data )\n",
    "yacht_target = np.array( targ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Call Regression Fit Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_OUTPUT_DIM = 1\n",
    "num_lay = int( input(\"How many hidden layers do you want (must be an integer)? \") )\n",
    "\n",
    "num_nodes = [ ]\n",
    "for i in np.linspace(1, num_lay, num_lay):\n",
    "    print(\"How many nodes in hidden layer \",int(i),\"? \")\n",
    "    num_nodes.append(int( input() ) )\n",
    "num_nodes.append( DATA_OUTPUT_DIM )\n",
    "\n",
    "#weights = construct_weights(DATA_INPUT_DIM, num_lay, num_nodes)\n",
    "\n",
    "#iris = datasets.load_iris()\n",
    "\n",
    "\n",
    "norm_data = preprocessing.normalize(yacht_data)\n",
    "norm_mpg  = np.array([ (x - min(mpg))/(max(mpg) - min(mpg)) for x in mpg ])\n",
    "norm_targ = preprocessing.normalize(yacht_target.reshape(-1, 1))\n",
    "data = np.zeros(( len(norm_data), len(norm_data[0])+1 )) - 1\n",
    "data[:,:-1] = norm_data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(norm_data, norm_targ, \n",
    "                                    test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                    test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = X_train.astype('float')\n",
    "X_val   = X_val.astype('float')\n",
    "X_test  = X_test.astype('float')\n",
    "y_train = y_train.astype('float')\n",
    "y_val   = y_val.astype('float')\n",
    "y_test  = y_test.astype('float')\n",
    "\n",
    "Net = net(X_train, y_train, num_lay, num_nodes,update_type='batch',\\\n",
    "          plot_figs=False, regression=True)\n",
    "#Net = net(X_train, y_train, num_lay, num_nodes,update_type='batch',\\\n",
    "#           plot_figs=True,eta_decay=True)\n",
    "#Net = net(X_train, y_train, num_lay, num_nodes,update_type='sequential',\\\n",
    "#           plot_figs=True, eta_decay=True)\n",
    "#Net = net(X_train, y_train, num_lay, num_nodes,update_type='sequential',\\\n",
    "#          momentum=True,plot_figs=True)\n",
    "Net.train_net(X_val, y_val)\n",
    "\n",
    "#train_net(data, targets, DATA_INPUT_DIM, num_lay, num_nodes)\n",
    "#print(\"X_val =\",X_val)\n",
    "#print(\"y_val =\",y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h = np.array([-0.14336014, -0.14410926, -0.14408688, -0.14324893, -0.14316859, -0.14316122,\n",
    "              -0.14373194, -0.1435684,  -0.14401371, -0.14393119, -0.14295337, -0.14376988,\n",
    "              -0.14317934, -0.1430634,  -0.14319975, -0.14332136, -0.1439913,  -0.14398813])\n",
    "hip = np.array([[1],[2],[3]])\n",
    "hippi = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "np.dot( h, hip[0][0] )\n",
    "#np.dot( h, hippi[0] )\n",
    "\n",
    "hippi[0].ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gross_file = open('yacht_data.txt','r')\n",
    "ugly_lines = gross_file.readlines()\n",
    "\n",
    "f1, f2, f3, f4, f5, f6, target = [], [], [], [], [], [], []\n",
    "\n",
    "print(\"1\",ugly_lines[1].split(' ')[1],\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gross_file = open('yacht_data.txt','r')\n",
    "ugly_lines = gross_file.readlines()\n",
    "\n",
    "f1, f2, f3, f4, f5, f6, target = [], [], [], [], [], [], []\n",
    "\n",
    "for lin in ugly_lines:\n",
    "    f1.append( float(lin.split(' ')[0] ) )\n",
    "    f2.append( float(lin.split(' ')[1] ) )\n",
    "    f3.append( float(lin.split(' ')[2] ) )\n",
    "    f4.append( float(lin.split(' ')[3] ) )\n",
    "    f5.append( float(lin.split(' ')[4] ) )\n",
    "    f6.append( float(lin.split(' ')[5] ) )\n",
    "    targ.append( float((lin.split(' ')[6]).split('\\n')[0]) )\n",
    "        \n",
    "\n",
    "yacht_data = pd.DataFrame( {\n",
    "                           'f1':f1, \n",
    "                           'f2':f2, \n",
    "                           'f3':f3,\n",
    "                           'f4':f4, \n",
    "                           'f5':f5, \n",
    "                           'f6':f6\n",
    "                           } )\n",
    "\n",
    "yacht_train  = np.array( yacht_data )\n",
    "yacht_target = np.array( targ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "twos = np.zeros(( 3,3 )) + 2\n",
    "thre = np.zeros(( 3,3 )) + 3\n",
    "four = np.zeros(( 3,3 )) + 4\n",
    "five = np.zeros(( 3,3 )) + 5\n",
    "sixs = np.zeros(( 3,3 )) + 6\n",
    "\n",
    "tw_lst, thr_lst, fr_lst, fv_lst, sx_lst = [], [], [], [], []\n",
    "\n",
    "for i in np.arange(5):\n",
    "    tw_lst.append( twos )\n",
    "    thr_lst.append( thre )\n",
    "    fr_lst.append( four )\n",
    "    fv_lst.append( five )\n",
    "    sx_lst.append( sixs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "w1 = np.array([[-.5,.1,1],[.25,.8,-1]])\n",
    "w2 = np.array([-.15,.72,.3])\n",
    "dat = np.array([[-1, -3,.5],\n",
    "                [-1,  8, 2],\n",
    "                [-1,  7,-2],\n",
    "                [-1,.25,.1]])\n",
    "hip = np.array([ np.dot(dat,w1[x]) for x in np.arange(len(w1)) ])\n",
    "\n",
    "aw = g_func( hip )\n",
    "g_func( hip )\n",
    "\n",
    "tes = list( dat[:,::-1][:,:1].flat )\n",
    "k = None\n",
    "type( k )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lst = [1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10]\n",
    "hip = [1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2]\n",
    "lst.insert(0, .5)\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "2+2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
